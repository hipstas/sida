{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import attk\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import librosa\n",
    "import timeit\n",
    "import random\n",
    "import subprocess\n",
    "import unicodecsv\n",
    "import urllib2\n",
    "from sklearn.externals import joblib\n",
    "from numpy import ma\n",
    "from aubio import source, pitch\n",
    "from moviepy.audio.io import AudioFileClip\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "\n",
    "training_audio_dir_name = \"Bill_Clinton_Classifier\"\n",
    "\n",
    "classifier_dir_pathname = '/sharedfolder/' + training_audio_dir_name + '/'\n",
    "\n",
    "os.chdir(classifier_dir_pathname)\n",
    "\n",
    "labels_to_use = [\"Male\", \"Bill Clinton\", \"Female\"]\n",
    "\n",
    "label_dir_names = [item.replace(' ', '_') for item in labels_to_use]\n",
    "\n",
    "speaker_0_label, speaker_1_label, speaker_2_label = labels_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features(dir_path):\n",
    "    features = []\n",
    "    for filename in [item for item in os.listdir(dir_path) if '.csv' in item]:\n",
    "        with open(os.path.join(dir_path, filename)) as fi:\n",
    "            csv_reader = csv.reader(fi)\n",
    "            for row in csv_reader:\n",
    "                features.append([float(item) for item in row])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load saved features\n",
    "\n",
    "speaker_1_features = load_features('/sharedfolder/Bill_Clinton_Classifier/Bill_Clinton/_vowel_mfccs_and_deltas')\n",
    "print(len(speaker_1_features))\n",
    "\n",
    "aapb_ubm_male_features = load_features('/sharedfolder/Bill_Clinton_Classifier/Male/_vowel_mfccs_and_deltas')\n",
    "print(len(aapb_ubm_male_features))\n",
    "\n",
    "aapb_ubm_female_features = load_features('/sharedfolder/Bill_Clinton_Classifier/Female/_vowel_mfccs_and_deltas')\n",
    "print(len(aapb_ubm_female_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Printing MFCCs and deltas for a single frame\n",
    "\n",
    "print(random.choice(speaker_1_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Combining feature sets\n",
    "\n",
    "#speaker_1_features = random.sample(speaker_1_features, 10000)\n",
    "\n",
    "speaker_0_features = aapb_ubm_male_features\n",
    "\n",
    "speaker_2_features = aapb_ubm_female_features\n",
    "\n",
    "#print(len(speaker_1_features))\n",
    "#print(len(ubm_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training and multi-layer perceptron model with 9/10 of training data and evaluating performance on remaining 1/10\n",
    "\n",
    "os.chdir(classifier_dir_pathname)\n",
    "\n",
    "import random\n",
    "#random.shuffle(speaker_0_features)\n",
    "#random.shuffle(speaker_1_features)\n",
    "#random.shuffle(speaker_2_features)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = speaker_0_features[:-len(speaker_0_features)/10] + speaker_1_features[:-len(speaker_1_features)/10] + speaker_2_features[:-len(speaker_2_features)/10]\n",
    "y = [1]*len(speaker_0_features[:-len(speaker_0_features)/10]) + [0]*len(speaker_1_features[:-len(speaker_1_features)/10]) + [2]*len(speaker_2_features[:-len(speaker_2_features)/10])\n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(y)\n",
    "\n",
    "X_test = speaker_0_features[-len(speaker_0_features)/10:] + speaker_1_features[-len(speaker_1_features)/10:] + speaker_2_features[-len(speaker_2_features)/10:]\n",
    "y_test = [1]*len(speaker_0_features[-len(speaker_0_features)/10:]) + [0]*len(speaker_1_features[-len(speaker_1_features)/10:]) + [2]*len(speaker_2_features[-len(speaker_2_features)/10:])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "    \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#classifier = MLPClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "classifier = MLPClassifier(max_iter = 2000, random_state = 9, \\\n",
    "                          hidden_layer_sizes = (100, 100), solver = 'adam', \\\n",
    "                          activation = 'relu').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(classifier.score(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training and saving an MLP model with all training data\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = speaker_0_features + speaker_1_features + speaker_2_features\n",
    "y = [0]*len(speaker_0_features) + [1]*len(speaker_1_features) + [2]*len(speaker_2_features)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "#classifier = MLPClassifier().fit(X_scaled, y)\n",
    "\n",
    "classifier = MLPClassifier(max_iter = 2000, random_state = 9, \\\n",
    "                          hidden_layer_sizes = (100, 100), solver = 'adam', \\\n",
    "                          activation = 'relu').fit(X_scaled, y)\n",
    "\n",
    "\n",
    "trained_model_filename = 'Bill_Clinton_vowels_mlpc_4096_100-16K_w_genders_scaled.pkl'\n",
    "scaler_filename = 'Bill_Clinton_vowels_mlpc_4096_100-16K_w_genders_scaled.scaler'\n",
    "\n",
    "print(trained_model_filename)\n",
    "print(scaler_filename)\n",
    "\n",
    "## Saving trained model and scaler\n",
    "joblib.dump(classifier, trained_model_filename)\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "classifier = joblib.load(trained_model_filename)\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "#### Start here to load pre-trained model ####\n",
    "##############################################\n",
    "\n",
    "trained_model_filename = 'Bill_Clinton_vowels_mlpc_4096_100-16K_w_genders_scaled.pkl'\n",
    "scaler_filename = 'Bill_Clinton_vowels_mlpc_4096_100-16K_w_genders_scaled.scaler'\n",
    "\n",
    "os.chdir(classifier_dir_pathname)\n",
    "classifier = joblib.load(trained_model_filename)\n",
    "scaler = joblib.load(scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "## Download unseen audio and split into 3-second WAV clips for testing\n",
    "import subprocess\n",
    "os.chdir(classifier_dir_pathname)\n",
    "\n",
    "try: os.mkdir('test_clips/')\n",
    "except: pass\n",
    "\n",
    "os.chdir(os.path.join(classifier_dir_pathname, 'test_clips'))\n",
    "\n",
    "#mp3_url = \"https://archive.org/download/GreatSpeechesAndInterviewsWithPresidentBillClintonAndOthers/GSI071110A_64kb.mp3\"\n",
    "mp3_url = \"http://www.stephenmclaughlin.net/hipstas/misc/nc6j0201.mp3\"\n",
    "\n",
    "mp3_filename = os.path.basename(mp3_url)\n",
    "\n",
    "wav_filename = mp3_filename[:-4]+'.wav'\n",
    "\n",
    "subprocess.call(['wget', '-N', mp3_url])\n",
    "\n",
    "subprocess.call(['ffmpeg', '-i', mp3_filename, wav_filename])\n",
    "\n",
    "subprocess.call(['ffmpeg', '-i', wav_filename, '-f', 'segment', '-segment_time', '3',  wav_filename[:-4] + '_3_sec_%04d.wav'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def most_common_class(class_ids):\n",
    "    mode_id = int(list(scipy.stats.mode(class_ids))[0][0])\n",
    "    #mode_id = 1  #####################################################################################\n",
    "    mode_id_percentage = float(float(class_ids.count(mode_id))/len(class_ids))\n",
    "    return (mode_id, mode_id_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Classifying short clips\n",
    "#### Repeat this cell several times to help choose a classifier threshold value.\n",
    "\n",
    "import scipy\n",
    "\n",
    "os.chdir(os.path.join(classifier_dir_pathname, 'test_clips'))\n",
    "\n",
    "wav_pathname = os.path.abspath(random.choice([item for item in os.listdir('./') if '3_sec' in item]))\n",
    "\n",
    "test_features = np.array(attk.get_mfccs_and_deltas(wav_pathname, n_mfcc=30, n_fft=4096, freq_min=100, freq_max=16000))\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "print(wav_pathname)\n",
    "\n",
    "results = classifier.predict(test_features)  ## Predicting new observation\n",
    "results_proba = classifier.predict_proba(test_features)  ## Predicting new observation\n",
    "\n",
    "print(results)\n",
    "print([round(max(item), 4) for item in list(results_proba)])\n",
    "\n",
    "vowel_results=[]\n",
    "\n",
    "vowel_bools = attk.get_vowel_segments(wav_pathname)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    try:\n",
    "        if vowel_bools[i]==True:\n",
    "            vowel_results.append(results[i])\n",
    "    except: pass\n",
    "            \n",
    "display(Audio(wav_pathname))\n",
    "\n",
    "print(\"MODE: \" + str(list(scipy.stats.mode(results))[0][0])) \n",
    "print(\"MODE vowels only: \" + str(list(scipy.stats.mode(vowel_results))[0][0])) ## Vowels only\n",
    "#print(\"All samples: \"+str(np.mean(results)))\n",
    "#print(\"Vowels only: \"+str(np.mean(vowel_results)))\n",
    "\n",
    "mode_id, mode_id_percentage = most_common_class(vowel_results)\n",
    "top_label = labels_to_use[mode_id]\n",
    "\n",
    "print('')\n",
    "print(\"Speaker: \" + str(top_label))\n",
    "print(\"Confidence: \" + str(mode_id_percentage))\n",
    "\n",
    "print('')\n",
    "\n",
    "print(str(mode_id) +','+ str(mode_id_percentage) + ',' + str(top_label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that classifies vowel segments only and returns \n",
    "## average output for the full clip\n",
    "\n",
    "## Function that classifies vowel segments only and returns \n",
    "## average output for the full clip\n",
    "\n",
    "def classify_clip(clip_pathname):\n",
    "    mfccs = np.array(attk.get_mfccs_and_deltas(clip_pathname,  n_mfcc=30, n_fft=4096, freq_min=100, freq_max=16000))\n",
    "    mfccs = scaler.transform(mfccs)\n",
    "    results = classifier.predict(mfccs)  ## Predicting new observation\n",
    "    vowel_results=[]\n",
    "    vowel_bools = attk.get_vowel_segments(clip_pathname)\n",
    "    #print(len(mfccs))\n",
    "    #print(len(results))\n",
    "    #print(len(vowel_bools))\n",
    "    \n",
    "    if len(vowel_bools)==0:\n",
    "            return most_common_class(results)\n",
    "        \n",
    "    for i in range(len(results)):\n",
    "        if vowel_bools[i]==True:\n",
    "            vowel_results.append(results[i])\n",
    "\n",
    "    return  most_common_class(vowel_results)[1]   ###### Just the percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "def most_common_class(class_ids):\n",
    "    mode_id = int(list(scipy.stats.mode(class_ids))[0][0])\n",
    "    mode_id = 1  #####################################################################################\n",
    "    mode_id_percentage = float(float(class_ids.count(mode_id))/len(class_ids))\n",
    "    return (mode_id, mode_id_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "## Classifying a long audio file\n",
    "\n",
    "## Writing classification output to CSV\n",
    "\n",
    "classifier_threshold = 0.0      # Classifier values below this threshold will be discarded\n",
    "\n",
    "os.chdir(os.path.join(classifier_dir_pathname, 'test_clips'))\n",
    "\n",
    "resolution_secs = 3.0\n",
    "\n",
    "media_filename = 'nc6j0201.wav'\n",
    "\n",
    "media_path = os.path.join(classifier_dir_pathname, 'test_clips', media_filename)\n",
    "\n",
    "csv_path = media_path[:-4]+'_Bill_Clinton_mlpc_4096_100-16K_w_genders_scaled_'+str(resolution_secs)+'s.csv'\n",
    "\n",
    "counter=0\n",
    "\n",
    "\n",
    "import timeit\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "snd = AudioFileClip.AudioFileClip(media_path)\n",
    "\n",
    "classifications = []\n",
    "\n",
    "for i in range(int(attk.duration(media_path)/resolution_secs)):\n",
    "    try:\n",
    "        snd.subclip(i * resolution_secs , (i * resolution_secs) + resolution_secs).write_audiofile('/tmp/temp_clip.wav')\n",
    "        value = classify_clip('/tmp/temp_clip.wav')\n",
    "        \n",
    "        with open(csv_path,'a') as fo:\n",
    "            duration = resolution_secs\n",
    "\n",
    "            if value >= classifier_threshold:\n",
    "                start = i * resolution_secs\n",
    "                fo.write(str(start) + ',' + str(duration) +','+ str(value) + ',' + speaker_1_label + '\\n')\n",
    "        \n",
    "    except:\n",
    "        classifications.append(0.0)\n",
    "        print(\"Error: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Time elapsed: \"+str(timeit.default_timer() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Writing classification output to CSV\n",
    "\n",
    "classifier_threshold = 0.2      # Classifier values below this threshold will be discarded\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier')\n",
    "\n",
    "csv_path = media_path[:-4]+'_mlpc2048_labels_'+str(resolution_secs)+'s.csv'\n",
    "\n",
    "counter=0\n",
    "\n",
    "with open(csv_path,'w') as fo:\n",
    "    duration = resolution_secs\n",
    "    for value in classifications:\n",
    "        if value >= classifier_threshold:\n",
    "            start = counter * resolution_secs\n",
    "            fo.write(str(start) + ',' + str(duration) +','+ str(value) + ',' + speaker_1_label + '\\n')\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
