{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install the latest version of attk (if necessary)\n",
    "#!pip install -U git+git://github.com/hipstas/audio-tagging-toolkit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory '/sharedfolder/GitHub/sida/___training_audio': File exists\r\n"
     ]
    }
   ],
   "source": [
    "import attk\n",
    "import os\n",
    "import numpy as np\n",
    "import timeit\n",
    "import random\n",
    "from sklearn.externals import joblib\n",
    "from numpy import ma\n",
    "from aubio import source, pitch\n",
    "import librosa\n",
    "from IPython.display import display, Audio\n",
    "import subprocess\n",
    "import unicodecsv\n",
    "import urllib2\n",
    "import csv\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/')\n",
    "\n",
    "!mkdir /sharedfolder/GitHub/sida/___training_audio\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio')\n",
    "\n",
    "## Uncomment lines below to download audio files for training\n",
    "\n",
    "#!wget http://www.stephenmclaughlin.net/HILT/audio_corpora/NPR_Fresh_Air_diarized.zip\n",
    "#!unzip NPR_Fresh_Air_diarized.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading label data CSV as a list of lists\n",
    "\n",
    "## Random 1-second labels\n",
    "#csv_url = \"https://raw.githubusercontent.com/hipstas/aapb-labels/master/Terry_Gross/Terry_Gross_labels.csv\"\n",
    "\n",
    "## Labels done by hand in Sonic Visualiser\n",
    "csv_url = 'https://raw.githubusercontent.com/hipstas/aapb-labels/master/Terry_Gross/Terry_Gross_corrected_SV.csv'\n",
    "\n",
    "csv_string = urllib2.urlopen(csv_url)\n",
    "\n",
    "train_table = []\n",
    "\n",
    "csv_reader = unicodecsv.reader(csv_string)\n",
    "\n",
    "for row in csv_reader:\n",
    "        train_table.append(row)\n",
    "\n",
    "train_table[:10]+['...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing header row if present\n",
    "\n",
    "if 'Media file basename' in train_table[0]:\n",
    "    train_table = train_table[1:]\n",
    "\n",
    "random.shuffle(train_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## Excerpting WAV clips corresponding to labels\n",
    "\n",
    "training_audio_pathname = \"NPR_Fresh_Air_diarized\"\n",
    "out_dir = '_classes_SV_' + training_audio_pathname\n",
    "\n",
    "\n",
    "for row in train_table:\n",
    "    try:\n",
    "        basename , start, duration, class_name, labeled_by = row  ## Assigning values in row to variables\n",
    "        filename = str(basename + '.mp3')\n",
    "        start = float(start)\n",
    "        end = float(start) + float(duration)\n",
    "        out_pathname = str(os.path.join(out_dir, class_name.replace(' ','_')))\n",
    "        try: \n",
    "            subprocess.call(['mkdir', '-p', out_pathname])\n",
    "        except:\n",
    "            pass\n",
    "        attk.subclip(os.path.join(training_audio_pathname, filename), float(start), end, out_pathname) ## <- attk\n",
    "    except Exception as e: \n",
    "        print(row)\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_pairs(media_path,vowel_time_ranges):\n",
    "    snd = AudioFileClip.AudioFileClip(media_path)\n",
    "    file_duration = attk.duration(media_path)\n",
    "    for pair in vowel_time_ranges:\n",
    "        start, end = pair\n",
    "        start = float(start)\n",
    "        end = float(end)\n",
    "        if end-start >= 0.1:  ## Ignore clips shorter than 0.1 second\n",
    "            basename = media_path.split('/')[-1][:-4]\n",
    "            out_filename = basename+'__'+str(round(start, 4))+'_'+str(round(end, 4))+'.wav'\n",
    "            snd.subclip(start, end).write_audiofile(os.path.join('_vowel_clips',out_filename))\n",
    "\n",
    "\n",
    "def batch_extract_vowels(media_dir):\n",
    "\n",
    "    starting_location = os.getcwd()\n",
    "    \n",
    "    os.chdir(media_dir)\n",
    "\n",
    "    bin_2048_to_sec_constant = 0.046439909297052155\n",
    "    \n",
    "    try: os.mkdir('_vowel_clips')\n",
    "    except: pass\n",
    "\n",
    "    filenames=[item for item in os.listdir('./') if item[-4:].lower() in ('.mp3','.wav','.mp4')]\n",
    "\n",
    "    tic=timeit.default_timer()\n",
    "\n",
    "    for filename in filenames[::-1]:\n",
    "        try:\n",
    "            vowel_bools = attk.get_vowel_segments(filename)\n",
    "\n",
    "            vowel_bin_ranges = attk.labels_to_ranges(vowel_bools, label=True)\n",
    "\n",
    "            vowel_time_ranges = [(s*bin_2048_to_sec_constant, e*bin_2048_to_sec_constant) for s, e in vowel_bin_ranges]\n",
    "            \n",
    "            extract_pairs(filename,vowel_time_ranges)\n",
    "\n",
    "        except: print(\"ERROR: \"+filename)\n",
    "\n",
    "    print(\"Time elapsed: \"+str(timeit.default_timer() - tic))\n",
    "\n",
    "    os.chdir(starting_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_SV_NPR_Fresh_Air_diarized')\n",
    "\n",
    "#for class_dir_name in [item for item in os.listdir('./') if os.path.isdir(item)]:\n",
    "#    batch_extract_vowels(class_dir_name)\n",
    "\n",
    "batch_extract_vowels('Terry_Gross')\n",
    "#batch_extract_vowels('Background_Speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_NPR_Fresh_Air_diarized')\n",
    "\n",
    "#for class_dir_name in [item for item in os.listdir('./') if os.path.isdir(item)]:\n",
    "#    batch_extract_vowels(class_dir_name)\n",
    "\n",
    "batch_extract_vowels('Terry_Gross')\n",
    "batch_extract_vowels('Background_Speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_ubm_clips_final_July_2017')\n",
    "\n",
    "#batch_extract_vowels('Female')\n",
    "batch_extract_vowels('Male')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting features\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_SV_NPR_Fresh_Air_diarized/')\n",
    "\n",
    "\n",
    "dir_names = [item for item in os.listdir('./') if os.path.isdir(item)]\n",
    "\n",
    "for dir_name in dir_names:\n",
    "    \n",
    "    os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_SV_NPR_Fresh_Air_diarized/' + dir_name + '/_vowel_clips')\n",
    "    \n",
    "    try: os.mkdir('../_vowel_mfccs_and_deltas')\n",
    "    except: pass\n",
    "    \n",
    "    for filename in [item for item in os.listdir('./') if item.lower()[-4:] == '.wav']:\n",
    "        csv_out_path = '../_vowel_mfccs_and_deltas/' + filename[:-4] + '.mfcc.csv'\n",
    "        if not os.path.isfile(csv_out_path):\n",
    "            try:\n",
    "                mfccs = attk.get_mfccs_and_deltas(filename)\n",
    "                with open(csv_out_path, 'w') as fo:\n",
    "                    csv_writer = csv.writer(fo)\n",
    "                    csv_writer.writerows(mfccs)  \n",
    "            except:\n",
    "                \"ERROR on \" + filename\n",
    "\n",
    "\n",
    "print(timeit.default_timer() - tic)\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting features\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_NPR_Fresh_Air_diarized/')\n",
    "\n",
    "dir_names = [item for item in os.listdir('./') if os.path.isdir(item)]\n",
    "\n",
    "for dir_name in dir_names:\n",
    "    \n",
    "    try:\n",
    "        os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_NPR_Fresh_Air_diarized/' + dir_name + '/_vowel_clips')\n",
    "    \n",
    "        try: os.mkdir('../_vowel_mfccs_and_deltas')\n",
    "        except: pass\n",
    "    \n",
    "        csv_out_path = '../_vowel_mfccs_and_deltas/' + filename[:-4] + '.mfcc.csv'\n",
    "        if not os.path.isfile(csv_out_path):\n",
    "            try:\n",
    "                mfccs = attk.get_mfccs_and_deltas(filename)\n",
    "                with open(csv_out_path, 'w') as fo:\n",
    "                    csv_writer = csv.writer(fo)\n",
    "                    csv_writer.writerows(mfccs)  \n",
    "            except:\n",
    "                \"ERROR on \" + filename\n",
    "                \n",
    "    except: pass\n",
    "\n",
    "print(timeit.default_timer() - tic)\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting features\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_ubm_clips_final_July_2017')\n",
    "\n",
    "dir_names = [item for item in os.listdir('./') if os.path.isdir(item)]\n",
    "\n",
    "for dir_name in dir_names:\n",
    "    \n",
    "    try:\n",
    "        os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_ubm_clips_final_July_2017/' + dir_name + '/_vowel_clips')\n",
    "    \n",
    "        try: os.mkdir('../_vowel_mfccs_and_deltas')\n",
    "        except: pass\n",
    "    \n",
    "        for filename in [item for item in os.listdir('./') if item.lower()[-4:] == '.wav']:\n",
    "            mfccs = attk.get_mfccs_and_deltas(filename)\n",
    "            with open('../_vowel_mfccs_and_deltas/' + filename[:-4] + '.mfcc.csv', 'w') as fo:\n",
    "                csv_writer = csv.writer(fo)\n",
    "                csv_writer.writerows(mfccs)  \n",
    "    except: pass\n",
    "\n",
    "print(timeit.default_timer() - tic)\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_NPR_Fresh_Air_diarized/Terry_Gross/_vowel_mfccs_and_deltas')\n",
    "\n",
    "gross_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            gross_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(gross_features))\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_SV_NPR_Fresh_Air_diarized/Terry_Gross/_vowel_mfccs_and_deltas')\n",
    "\n",
    "gross_sv_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            gross_sv_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(gross_sv_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_NPR_Fresh_Air_diarized/Background_Speaker/_vowel_mfccs_and_deltas')\n",
    "\n",
    "fa_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            fa_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(fa_ubm_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_ubm_clips_final_July_2017/Male/_vowel_mfccs_and_deltas')\n",
    "\n",
    "m_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            m_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(m_ubm_features))\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/_classes_ubm_clips_final_July_2017/Female/_vowel_mfccs_and_deltas')\n",
    "\n",
    "f_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            f_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(f_ubm_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/sharedfolder/GitHub/sida/___training_audio/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining feature sets\n",
    "\n",
    "speaker_1_mfccs = gross_features #+ random.sample(gross_sv_features, 1000)\n",
    "bg_mfccs = fa_ubm_features + m_ubm_features + f_ubm_features\n",
    "\n",
    "#speaker_1_mfccs = terry_gross_mfccs[::-1]\n",
    "#bg_mfccs = ubm_mfccs[::-1]\n",
    "\n",
    "random.shuffle(speaker_1_mfccs)\n",
    "random.shuffle(bg_mfccs)\n",
    "\n",
    "print(len(speaker_1_mfccs))\n",
    "print(len(ubm_mfccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision tree\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "X = speaker_1_mfccs[:-len(speaker_1_mfccs)/10] + ubm_mfccs[:-len(ubm_mfccs)/10]\n",
    "y = [0]*len(speaker_1_mfccs[:-len(speaker_1_mfccs)/10]) + [1]*len(ubm_mfccs[:-len(ubm_mfccs)/10])\n",
    "\n",
    "X_test = speaker_1_mfccs[-len(speaker_1_mfccs)/10:] + ubm_mfccs[-len(ubm_mfccs)/10:]\n",
    "y_test = [0]*len(speaker_1_mfccs[-len(speaker_1_mfccs)/10:]) + [1]*len(ubm_mfccs[-len(ubm_mfccs)/10:])\n",
    "\n",
    "#classifier = ExtraTreesClassifier().fit(X, y)\n",
    "classifier = MLPClassifier().fit(X, y)\n",
    "\n",
    "## Saving trained model\n",
    "joblib.dump(classifier,'gross_vowels_mlpc_2048.pkl')\n",
    "classifier = joblib.load('gross_vowels_mlpc_2048.pkl')\n",
    "\n",
    "print(timeit.default_timer() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = joblib.load('gross_vowels_mlpc_2048.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining feature sets\n",
    "\n",
    "speaker_1_mfccs = gross_features #+ random.sample(gross_sv_features, 20000)\n",
    "bg_mfccs = fa_ubm_features + m_ubm_features + f_ubm_features\n",
    "\n",
    "#speaker_1_mfccs = terry_gross_mfccs[::-1]\n",
    "#bg_mfccs = ubm_mfccs[::-1]\n",
    "\n",
    "#random.shuffle(speaker_1_mfccs)\n",
    "#random.shuffle(bg_mfccs)\n",
    "\n",
    "print(len(speaker_1_mfccs))\n",
    "print(len(ubm_mfccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gross_sv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KERAS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "speaker_1_mfccs_df = pd.DataFrame(speaker_1_mfccs[:-500])\n",
    "\n",
    "speaker_1_mfccs_df['Class Label'] = 1.0   ## Terry Gross\n",
    "\n",
    "\n",
    "bg_mfccs_df = pd.DataFrame(bg_mfccs[:-500])\n",
    "\n",
    "bg_mfccs_df['Class Label'] = 0.0    ## Background speaker\n",
    "\n",
    "\n",
    "training_data_df = pd.concat([speaker_1_mfccs_df, bg_mfccs_df])\n",
    "\n",
    "training_data_df.head()  \n",
    "\n",
    "\n",
    "\n",
    "## Test fold\n",
    "speaker_1_mfccs_df_test = pd.DataFrame(speaker_1_mfccs[-500:])\n",
    "speaker_1_mfccs_df_test['Class Label'] = 1.0   ## Terry Gross\n",
    "\n",
    "bg_mfccs_df_test = pd.DataFrame(bg_mfccs[-500:])\n",
    "bg_mfccs_df_test['Class Label'] = 0.0    ## Background speaker\n",
    "\n",
    "test_data_df = pd.concat([speaker_1_mfccs_df_test, bg_mfccs_df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "scaled_training = scaler.fit_transform(training_data_df)\n",
    "scaled_testing = scaler.transform(test_data_df)\n",
    "\n",
    "# Print out the adjustment that the scaler applied to the total_earnings column of data\n",
    "#######print(\"Note: total_earnings values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[8], scaler.min_[8]))\n",
    "\n",
    "# Create new pandas DataFrame objects from the scaled data\n",
    "scaled_training_df = pd.DataFrame(scaled_training, columns=training_data_df.columns.values)\n",
    "scaled_testing_df = pd.DataFrame(scaled_testing, columns=test_data_df.columns.values)\n",
    "\n",
    "scaled_testing_df.head()\n",
    "\n",
    "\n",
    "# Save scaled dataframes to new CSV files\n",
    "scaled_training_df.to_csv(\"terry_gross_vs_ubm_train_scaled.csv\", index=False)\n",
    "scaled_testing_df.to_csv(\"terry_gross_vs_ubm_test_scaled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tic=timeit.default_timer()\n",
    "\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "training_data_df = pd.read_csv(\"terry_gross_vs_ubm_train_scaled.csv\")\n",
    "\n",
    "X = training_data_df.drop('Class Label', axis=1).values\n",
    "Y = training_data_df[['Class Label']].values\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=38, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(81, activation='relu'))\n",
    "model.add(Dense(42, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "\n",
    "print(timeit.default_timer() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Load the separate test data set\n",
    "test_data_df = pd.read_csv(\"terry_gross_vs_ubm_test_scaled.csv\")\n",
    "\n",
    "X_test = test_data_df.drop('Class Label', axis=1).values\n",
    "Y_test = test_data_df[['Class Label']].values\n",
    "\n",
    "test_error_rate = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: \" + str(test_error_rate))\n",
    "\n",
    "# Save the model to disk\n",
    "model.save(\"trained_model.h5\")\n",
    "print(\"Model saved to disk.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def classify_clip(clip_pathname):\n",
    "    mfccs = attk.get_mfccs_and_deltas(clip_pathname)\n",
    "    results = classifier.predict(mfccs)  ## Predicting new observation\n",
    "    vowel_results=[]\n",
    "    vowel_bools = get_vowel_segments(clip_pathname)[::2]\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        if vowel_bools[i]==True:\n",
    "            vowel_results.append(results[i])\n",
    "\n",
    "    return np.mean(vowel_results) ## Vowels only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loading pre-trained model\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#random_forest=joblib.load('pesca_vowels_random_forest_2048.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Classifying short clips\n",
    "\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "filename = random.choice(os.listdir('unseen/'))\n",
    "test_pathname = 'unseen/'+filename\n",
    "test_mfccs=attk.get_mfccs_and_deltas(test_pathname)\n",
    "\n",
    "print(test_pathname)\n",
    "\n",
    "results = classifier.predict(test_mfccs)  ## Predicting new observation\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "vowel_results=[]\n",
    "\n",
    "vowel_bools = get_vowel_segments(test_pathname)[::2]\n",
    "\n",
    "for i in range(len(results)):\n",
    "    if vowel_bools[i]==True:\n",
    "        vowel_results.append(results[i])\n",
    "\n",
    "display(Audio(test_pathname))\n",
    "\n",
    "\n",
    "print(\"All: \"+str(np.mean(results)))\n",
    "print(\"Vowels only: \"+str(np.mean(vowel_results)))\n",
    "\n",
    "#print(\"Time elapsed: \"+str(timeit.default_timer() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(get_vowel_segments(test_pathname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!du /tmp/temp_clip.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Classifying a long audio file\n",
    "\n",
    "\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "\n",
    "media_path = \"/sharedfolder/fa_eval/FA_Author_Tom_Perrotta.mp3\"\n",
    "\n",
    "\n",
    "snd = AudioFileClip.AudioFileClip(media_path)\n",
    "\n",
    "classifications=[]\n",
    "\n",
    "for i in range(int(attk.duration(media_path))):\n",
    "#    try:\n",
    "        snd.subclip(i,i+1).write_audiofile('/tmp/temp_clip.wav')\n",
    "        classifications.append(classify_clip('/tmp/temp_clip.wav'))\n",
    "#    except: print('missed one')\n",
    "\n",
    "classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing classification output to CSV\n",
    "\n",
    "counter=0\n",
    "\n",
    "class_0_secs=[]\n",
    "class_1_secs=[]\n",
    "\n",
    "i=0\n",
    "\n",
    "for classification in attk.smooth(np.array(classifications)):\n",
    "    if classification < 0.34:\n",
    "        class_0_secs.append(i)\n",
    "    if classification > 0.38:\n",
    "        class_1_secs.append(i)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "counter=0\n",
    "\n",
    "csv_path=media_path[:-4]+'_extratrees2048_labels.csv'\n",
    "\n",
    "with open(csv_path,'w') as fo:\n",
    "    for pair in seconds_list_to_ranges(class_0_secs):\n",
    "        fo.write(str(float(pair[0]))+','+str(float(pair[1]))+',Terry Gross\\n')\n",
    "    for pair in seconds_list_to_ranges(class_1_secs):\n",
    "        fo.write(str(float(pair[0]))+','+str(float(pair[1]))+',Background\\n')\n",
    "\n",
    "\n",
    "print(timeit.default_timer() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
