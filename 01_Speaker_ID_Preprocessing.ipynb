{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIDA: Speaker Identification for Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import attk\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import librosa\n",
    "import timeit\n",
    "import random\n",
    "import subprocess\n",
    "import unicodecsv\n",
    "import urllib2\n",
    "from sklearn.externals import joblib\n",
    "from numpy import ma\n",
    "from aubio import source, pitch\n",
    "from moviepy.audio.io import AudioFileClip\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "!mkdir -p /sharedfolder/sida_classifier/training_set/\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/training_set/')\n",
    "\n",
    "## Download audio files for training\n",
    "#### (You may want to comment out the lines below once the download is complete.)\n",
    "!wget -N http://xtra.arloproject.com/datasets/audio/NPR_Fresh_Air_20_episodes.zip\n",
    "!unzip NPR_Fresh_Air_20_episodes.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download new 1-second labels\n",
    "\n",
    "csv_url = \"https://raw.githubusercontent.com/hipstas/podcast-speaker-labels/master/Fresh_Air/Terry_Gross_labels_randomized.csv\"\n",
    "\n",
    "csv_string = urllib2.urlopen(csv_url)\n",
    "\n",
    "train_table = []\n",
    "\n",
    "\n",
    "## Load CSV as list of lists\n",
    "\n",
    "csv_reader = unicodecsv.reader(csv_string)\n",
    "\n",
    "for row in csv_reader:\n",
    "        train_table.append(row)\n",
    "\n",
    "train_table[:10]+['...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Remove header row (if present)\n",
    "\n",
    "if 'Media file basename' in train_table[0]:\n",
    "    train_table = train_table[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## Excerpting WAV clips corresponding to labels\n",
    "#### (This may take a while.)\n",
    "\n",
    "training_audio_pathname = \"NPR_Fresh_Air_20_episodes\"\n",
    "out_dir = '_classes_' + training_audio_pathname\n",
    "\n",
    "for row in train_table:\n",
    "    try:\n",
    "        basename, start, duration, class_name, labeled_by = row  ## Assigning values in row to variables\n",
    "        filename = str(basename + '.mp3')\n",
    "        start = float(start)\n",
    "        end = float(start) + float(duration)\n",
    "        wav_out_pathname = str(os.path.join(out_dir, class_name.replace(' ','_')))\n",
    "        try: \n",
    "            subprocess.call(['mkdir', '-p', wav_out_pathname])\n",
    "        except:\n",
    "            pass\n",
    "        attk.subclip(os.path.join(training_audio_pathname, filename), float(start), end, wav_out_pathname) ## <- attk\n",
    "    except Exception as e: \n",
    "        print(row)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining functions we will use use below\n",
    "\n",
    "# Extract audio segments to WAV for an audio/video pathname and a list of 2-tuple time values\n",
    "def extract_vowel_pairs(media_path, vowel_time_ranges):\n",
    "    snd = AudioFileClip.AudioFileClip(media_path)\n",
    "    file_duration = attk.duration(media_path)\n",
    "    for pair in vowel_time_ranges:\n",
    "        start, end = pair\n",
    "        start = float(start)\n",
    "        end = float(end)\n",
    "        if end-start >= 0.1:  ## Ignore clips shorter than 0.1 second\n",
    "            basename = media_path.split('/')[-1][:-4]\n",
    "            out_filename = basename+'__'+str(round(start, 4))+'_'+str(round(end, 4))+'.wav'\n",
    "            out_pathname = os.path.join('_vowel_clips',out_filename)\n",
    "            if not os.path.isfile(out_pathname):\n",
    "                snd.subclip(start, end).write_audiofile(out_pathname)\n",
    "\n",
    "# Extract vowel segments to WAV for every audio/video file in a given directory\n",
    "def batch_extract_vowels(media_dir):\n",
    "    starting_location = os.getcwd()\n",
    "    os.chdir(media_dir)\n",
    "    bin_2048_to_sec_constant = 0.046439909297052155\n",
    "    try: os.mkdir('_vowel_clips')\n",
    "    except: pass\n",
    "    filenames=[item for item in os.listdir('./') if item[-4:].lower() in ('.mp3','.wav','.mp4')]\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            vowel_bools = attk.get_vowel_segments(filename)\n",
    "            vowel_bin_ranges = attk.labels_to_ranges(vowel_bools, label=True)\n",
    "            vowel_time_ranges = [(s*bin_2048_to_sec_constant, e*bin_2048_to_sec_constant) for s, e in vowel_bin_ranges]\n",
    "            extract_vowel_pairs(filename,vowel_time_ranges)\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: \" + filename)\n",
    "            print(e)\n",
    "    os.chdir(starting_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "## Extract vowel segments from labeled audio clips\n",
    "#### (This may take a while.)\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/training_set/_classes_NPR_Fresh_Air_20_episodes/')\n",
    "\n",
    "batch_extract_vowels('Terry_Gross')\n",
    "batch_extract_vowels('Background_Speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract features (MFCCs, deltas, and delta-deltas) from Terry Gross & UBM vowel clips, then write features to CSVs\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/training_set/_classes_NPR_Fresh_Air_20_episodes')\n",
    "\n",
    "dir_names = [item for item in os.listdir('./') if os.path.isdir(item)]\n",
    "\n",
    "for dir_name in dir_names:\n",
    "    try:\n",
    "        os.chdir('/sharedfolder/sida_classifier/training_set/_classes_NPR_Fresh_Air_20_episodes/' + dir_name + '/_vowel_clips')\n",
    "        try: os.mkdir('../_vowel_mfccs_and_deltas')\n",
    "        except: pass\n",
    "        filenames = os.listdir('./')\n",
    "        for filename in filenames:\n",
    "            csv_out_path = '../_vowel_mfccs_and_deltas/' + filename[:-4] + '.mfcc.csv'\n",
    "            if not os.path.isfile(csv_out_path):\n",
    "                try:\n",
    "                    mfccs = attk.get_mfccs_and_deltas(filename)\n",
    "                    with open(csv_out_path, 'w') as fo:\n",
    "                        csv_writer = csv.writer(fo)\n",
    "                        csv_writer.writerows(mfccs)  \n",
    "                except Exception as e:\n",
    "                    print('FILE ERROR: ' + filename)\n",
    "                    print(e)\n",
    "    except Exception as e:\n",
    "        print('SKIPPING DIRECTORY: ' + dir_name)     ## Skipping class directories for which we didn't extract vowels\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## Download and unzip prepared UBM feature set\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/')\n",
    "\n",
    "!wget -N https://github.com/hipstas/aapb-ubm/blob/master/UBM_feature_set/AAPB_female_vowel_mfccs_and_deltas.zip\n",
    "!wget -N https://github.com/hipstas/aapb-ubm/blob/master/UBM_feature_set/AAPB_male_vowel_mfccs_and_deltas.zip\n",
    "#!wget -N https://raw.githubusercontent.com/hipstas/shaping-humanities-data/master/feature_sets/Terry_Gross_vowel_mfccs_and_deltas.zip\n",
    "#!wget -N https://raw.githubusercontent.com/hipstas/shaping-humanities-data/master/feature_sets/Fresh_Air_ubm_vowel_mfccs_and_deltas.zip\n",
    "\n",
    "!unzip AAPB_female_vowel_mfccs_and_deltas.zip\n",
    "!unzip AAPB_male_vowel_mfccs_and_deltas.zip\n",
    "#!unzip Terry_Gross_vowel_mfccs_and_deltas.zip\n",
    "#!unzip Fresh_Air_ubm_vowel_mfccs_and_deltas.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load saved features\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/Terry_Gross_vowel_mfccs_and_deltas')\n",
    "\n",
    "gross_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            gross_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(gross_features))\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/Terry_Gross_vowel_mfccs_and_deltas')\n",
    "\n",
    "fresh_air_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            fresh_air_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(fresh_air_ubm_features))\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/AAPB_male_vowel_mfccs_and_deltas')\n",
    "\n",
    "m_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            m_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(m_ubm_features))\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/AAPB_female_vowel_mfccs_and_deltas')\n",
    "\n",
    "f_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            f_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(f_ubm_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Printing MFCCs and deltas for a single frame\n",
    "\n",
    "print(random.choice(gross_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Combining feature sets\n",
    "\n",
    "speaker_1_mfccs = gross_features\n",
    "ubm_mfccs = fresh_air_ubm_features + m_ubm_features + f_ubm_features\n",
    "\n",
    "print(len(speaker_1_mfccs))\n",
    "print(len(ubm_mfccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training Extra Trees Classifier\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/')\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "X = speaker_1_mfccs[:-len(speaker_1_mfccs)/10] + ubm_mfccs[:-len(ubm_mfccs)/10]\n",
    "y = [1]*len(speaker_1_mfccs[:-len(speaker_1_mfccs)/10]) + [0]*len(ubm_mfccs[:-len(ubm_mfccs)/10])\n",
    "\n",
    "X_test = speaker_1_mfccs[-len(speaker_1_mfccs)/10:] + ubm_mfccs[-len(ubm_mfccs)/10:]\n",
    "y_test = [1]*len(speaker_1_mfccs[-len(speaker_1_mfccs)/10:]) + [0]*len(ubm_mfccs[-len(ubm_mfccs)/10:])\n",
    "\n",
    "classifier = ExtraTreesClassifier().fit(X, y)\n",
    "\n",
    "## Saving trained model\n",
    "joblib.dump(classifier,'gross_vowels_extratrees_2048.pkl')\n",
    "classifier = joblib.load('gross_vowels_extratrees_2048.pkl')\n",
    "\n",
    "print(classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training Simple Multi-Layer Perceptron Model\n",
    "\n",
    "os.chdir('/sharedfolder/sida_classifier/')\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = speaker_1_mfccs[:-len(speaker_1_mfccs)/10] + ubm_mfccs[:-len(ubm_mfccs)/10]\n",
    "y = [1]*len(speaker_1_mfccs[:-len(speaker_1_mfccs)/10]) + [0]*len(ubm_mfccs[:-len(ubm_mfccs)/10])\n",
    "\n",
    "X_test = speaker_1_mfccs[-len(speaker_1_mfccs)/10:] + ubm_mfccs[-len(ubm_mfccs)/10:]\n",
    "y_test = [1]*len(speaker_1_mfccs[-len(speaker_1_mfccs)/10:]) + [0]*len(ubm_mfccs[-len(ubm_mfccs)/10:])\n",
    "\n",
    "#classifier = ExtraTreesClassifier().fit(X, y)\n",
    "classifier = MLPClassifier().fit(X, y)\n",
    "\n",
    "## Saving trained model\n",
    "joblib.dump(classifier,'gross_vowels_mlpc_2048.pkl')\n",
    "classifier = joblib.load('gross_vowels_mlpc_2048.pkl')\n",
    "\n",
    "print(classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
