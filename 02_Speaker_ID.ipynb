{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIDA: Speaker ID for Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import attk\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import librosa\n",
    "import timeit\n",
    "import random\n",
    "import subprocess\n",
    "import unicodecsv\n",
    "import urllib2\n",
    "from sklearn.externals import joblib\n",
    "from numpy import ma\n",
    "from aubio import source, pitch\n",
    "from moviepy.audio.io import AudioFileClip\n",
    "from IPython.display import display, Audio\n",
    "\n",
    "!mkdir -p /sharedfolder/_training_audio\n",
    "\n",
    "os.chdir('/sharedfolder/_training_audio/')\n",
    "\n",
    "## Download audio files for training\n",
    "\n",
    "#!wget -N http://www.stephenmclaughlin.net/HILT/audio_corpora/NPR_Fresh_Air_diarized.zip\n",
    "#!unzip NPR_Fresh_Air_diarized.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Download new 1-second labels\n",
    "\n",
    "csv_url = \"https://raw.githubusercontent.com/hipstas/aapb-labels/master/Speaker_labels_randomized/Terry_Gross/Terry_Gross_labels.csv\"\n",
    "\n",
    "csv_string = urllib2.urlopen(csv_url)\n",
    "\n",
    "train_table = []\n",
    "\n",
    "\n",
    "## Load CSV as list of lists\n",
    "\n",
    "csv_reader = unicodecsv.reader(csv_string)\n",
    "\n",
    "for row in csv_reader:\n",
    "        train_table.append(row)\n",
    "\n",
    "train_table[:10]+['...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Remove header row (if present)\n",
    "\n",
    "if 'Media file basename' in train_table[0]:\n",
    "    train_table = train_table[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## Excerpting WAV clips corresponding to labels\n",
    "\n",
    "training_audio_pathname = \"NPR_Fresh_Air_diarized\"\n",
    "out_dir = '_classes_' + training_audio_pathname\n",
    "\n",
    "for row in train_table:\n",
    "    try:\n",
    "        basename, start, duration, class_name, labeled_by = row  ## Assigning values in row to variables\n",
    "        filename = str(basename + '.mp3')\n",
    "        start = float(start)\n",
    "        end = float(start) + float(duration)\n",
    "        wav_out_pathname = str(os.path.join(out_dir, class_name.replace(' ','_')))\n",
    "        try: \n",
    "            subprocess.call(['mkdir', '-p', wav_out_pathname])\n",
    "        except:\n",
    "            pass\n",
    "        attk.subclip(os.path.join(training_audio_pathname, filename), float(start), end, wav_out_pathname) ## <- attk\n",
    "    except Exception as e: \n",
    "        print(row)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Defining functions we'll use use below\n",
    "\n",
    "# Extract audio segments to WAV for an audio/video pathname and a list of 2-tuple time values\n",
    "def extract_vowel_pairs(media_path, vowel_time_ranges):\n",
    "    snd = AudioFileClip.AudioFileClip(media_path)\n",
    "    file_duration = attk.duration(media_path)\n",
    "    for pair in vowel_time_ranges:\n",
    "        start, end = pair\n",
    "        start = float(start)\n",
    "        end = float(end)\n",
    "        if end-start >= 0.1:  ## Ignore clips shorter than 0.1 second\n",
    "            basename = media_path.split('/')[-1][:-4]\n",
    "            out_filename = basename+'__'+str(round(start, 4))+'_'+str(round(end, 4))+'.wav'\n",
    "            snd.subclip(start, end).write_audiofile(os.path.join('_vowel_clips',out_filename))\n",
    "\n",
    "# Extract vowel segments to WAV for every audio/video file in a given directory\n",
    "def batch_extract_vowels(media_dir):\n",
    "    starting_location = os.getcwd()\n",
    "    os.chdir(media_dir)\n",
    "    bin_2048_to_sec_constant = 0.046439909297052155\n",
    "    try: os.mkdir('_vowel_clips')\n",
    "    except: pass\n",
    "    filenames=[item for item in os.listdir('./') if item[-4:].lower() in ('.mp3','.wav','.mp4')]\n",
    "    for filename in filenames[::-1]:\n",
    "        try:\n",
    "            vowel_bools = attk.get_vowel_segments(filename)\n",
    "            vowel_bin_ranges = attk.labels_to_ranges(vowel_bools, label=True)\n",
    "            vowel_time_ranges = [(s*bin_2048_to_sec_constant, e*bin_2048_to_sec_constant) for s, e in vowel_bin_ranges]\n",
    "            extract_pairs(filename,vowel_time_ranges)\n",
    "        except: print(\"***** ERROR: \"+filename)\n",
    "    os.chdir(starting_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## Extract vowel segments from labeled audio clips\n",
    "\n",
    "os.chdir('/sharedfolder/_training_audio/_classes_NPR_Fresh_Air_diarized')\n",
    "\n",
    "batch_extract_vowels('Terry_Gross')\n",
    "batch_extract_vowels('Background_Speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract features from Terry Gross & UBM vowel clips & write CSVs (MFCCs, deltas, and deltad-deltas)\n",
    "\n",
    "os.chdir('/sharedfolder/_training_audio/_classes_NPR_Fresh_Air_diarized')\n",
    "\n",
    "dir_names = [item for item in os.listdir('./') if os.path.isdir(item)]\n",
    "\n",
    "for dir_name in dir_names:\n",
    "    \n",
    "    try:\n",
    "        os.chdir('/sharedfolder/_classes_NPR_Fresh_Air_diarized/' + dir_name + '/_vowel_clips')\n",
    "    \n",
    "        try: os.mkdir('../_vowel_mfccs_and_deltas')\n",
    "        except: pass\n",
    "    \n",
    "        csv_out_path = '../_vowel_mfccs_and_deltas/' + filename[:-4] + '.mfcc.csv'\n",
    "        if not os.path.isfile(csv_out_path):\n",
    "            try:\n",
    "                mfccs = attk.get_mfccs_and_deltas(filename)\n",
    "                with open(csv_out_path, 'w') as fo:\n",
    "                    csv_writer = csv.writer(fo)\n",
    "                    csv_writer.writerows(mfccs)  \n",
    "            except:\n",
    "                \"ERROR on \" + filename\n",
    "                \n",
    "    except: pass  ## Ignoring classes for which we haven't extracted vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "## Download and unzip prepared feature sets\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "\n",
    "!wget -N https://raw.githubusercontent.com/hipstas/shaping-humanities-data/master/feature_sets/Terry_Gross_vowel_mfccs_and_deltas.zip\n",
    "!wget -N https://raw.githubusercontent.com/hipstas/shaping-humanities-data/master/feature_sets/Fresh_Air_ubm_vowel_mfccs_and_deltas.zip\n",
    "!wget -N https://raw.githubusercontent.com/hipstas/shaping-humanities-data/master/feature_sets/AAPB_female_vowel_mfccs_and_deltas.zip\n",
    "!wget -N https://raw.githubusercontent.com/hipstas/shaping-humanities-data/master/feature_sets/AAPB_male_vowel_mfccs_and_deltas.zip\n",
    "\n",
    "!unzip Terry_Gross_vowel_mfccs_and_deltas.zip\n",
    "!unzip Fresh_Air_ubm_vowel_mfccs_and_deltas.zip\n",
    "!unzip AAPB_female_vowel_mfccs_and_deltas.zip\n",
    "!unzip AAPB_male_vowel_mfccs_and_deltas.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load saved features\n",
    "\n",
    "os.chdir('/sharedfolder/Terry_Gross_vowel_mfccs_and_deltas')\n",
    "\n",
    "gross_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            gross_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(gross_features))\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/Terry_Gross_vowel_mfccs_and_deltas')\n",
    "\n",
    "fresh_air_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            fresh_air_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(fresh_air_ubm_features))\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/AAPB_male_vowel_mfccs_and_deltas')\n",
    "\n",
    "m_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            m_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(m_ubm_features))\n",
    "\n",
    "\n",
    "os.chdir('/sharedfolder/AAPB_female_vowel_mfccs_and_deltas')\n",
    "\n",
    "f_ubm_features = []\n",
    "\n",
    "for filename in os.listdir('./'):\n",
    "    with open(filename) as fi:\n",
    "        csv_reader = csv.reader(fi)\n",
    "        for row in csv_reader:\n",
    "            f_ubm_features.append([float(item) for item in row])\n",
    "\n",
    "print(len(f_ubm_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Printing MFCCs and deltas for a single frame\n",
    "\n",
    "print(random.choice(gross_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Combining feature sets\n",
    "\n",
    "speaker_1_mfccs = gross_features\n",
    "ubm_mfccs = fresh_air_ubm_features + m_ubm_features + f_ubm_features\n",
    "\n",
    "print(len(speaker_1_mfccs))\n",
    "print(len(ubm_mfccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training Extra Trees Classifier\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "X = speaker_1_mfccs[:-len(speaker_1_mfccs)/10] + ubm_mfccs[:-len(ubm_mfccs)/10]\n",
    "y = [1]*len(speaker_1_mfccs[:-len(speaker_1_mfccs)/10]) + [0]*len(ubm_mfccs[:-len(ubm_mfccs)/10])\n",
    "\n",
    "X_test = speaker_1_mfccs[-len(speaker_1_mfccs)/10:] + ubm_mfccs[-len(ubm_mfccs)/10:]\n",
    "y_test = [1]*len(speaker_1_mfccs[-len(speaker_1_mfccs)/10:]) + [0]*len(ubm_mfccs[-len(ubm_mfccs)/10:])\n",
    "\n",
    "classifier = ExtraTreesClassifier().fit(X, y)\n",
    "\n",
    "## Saving trained model\n",
    "joblib.dump(classifier,'gross_vowels_extratrees_2048.pkl')\n",
    "classifier = joblib.load('gross_vowels_extratrees_2048.pkl')\n",
    "\n",
    "print(classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Training Simple Multi-Layer Perceptron Model\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = speaker_1_mfccs[:-len(speaker_1_mfccs)/10] + ubm_mfccs[:-len(ubm_mfccs)/10]\n",
    "y = [1]*len(speaker_1_mfccs[:-len(speaker_1_mfccs)/10]) + [0]*len(ubm_mfccs[:-len(ubm_mfccs)/10])\n",
    "\n",
    "X_test = speaker_1_mfccs[-len(speaker_1_mfccs)/10:] + ubm_mfccs[-len(ubm_mfccs)/10:]\n",
    "y_test = [1]*len(speaker_1_mfccs[-len(speaker_1_mfccs)/10:]) + [0]*len(ubm_mfccs[-len(ubm_mfccs)/10:])\n",
    "\n",
    "#classifier = ExtraTreesClassifier().fit(X, y)\n",
    "classifier = MLPClassifier().fit(X, y)\n",
    "\n",
    "## Saving trained model\n",
    "joblib.dump(classifier,'gross_vowels_mlpc_2048.pkl')\n",
    "classifier = joblib.load('gross_vowels_mlpc_2048.pkl')\n",
    "\n",
    "print(classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "#### Start here to load pre-trained model ####\n",
    "##############################################\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "#classifier = joblib.load('gross_vowels_mlpc_2048.pkl')\n",
    "#classifier = joblib.load('gross_vowels_extratrees_2048.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "## Download and unzip a set of 358 3-second Fresh Air clips\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "!wget -N https://github.com/hipstas/shaping-humanities-data/blob/master/audio/Fresh_Air_2017-07-31_3-sec_clips.zip?raw=true -O Fresh_Air_2017-07-31_3-sec_clips.zip\n",
    "!unzip Fresh_Air_2017-07-31_3-sec_clips.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Classifying short clips\n",
    "\n",
    "os.chdir('/sharedfolder/Fresh_Air_2017-07-31_3-sec_clips/')\n",
    "\n",
    "wav_pathname = os.path.abspath(random.choice(os.listdir('./')))\n",
    "\n",
    "test_mfccs = attk.get_mfccs_and_deltas(wav_pathname)\n",
    "\n",
    "print(wav_pathname)\n",
    "\n",
    "results = classifier.predict(test_mfccs)  ## Predicting new observation\n",
    "\n",
    "print(results)\n",
    "\n",
    "vowel_results=[]\n",
    "\n",
    "vowel_bools = attk.get_vowel_segments(wav_pathname)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    if vowel_bools[i]==True:\n",
    "        vowel_results.append(results[i])\n",
    "\n",
    "display(Audio(wav_pathname))\n",
    "\n",
    "print(\"All samples: \"+str(np.mean(results)))\n",
    "print(\"Vowels only: \"+str(np.mean(vowel_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that classifies *vowel segments only* and returns \n",
    "## average output for the full recording\n",
    "\n",
    "def classify_clip(clip_pathname):\n",
    "    mfccs = attk.get_mfccs_and_deltas(clip_pathname)\n",
    "    results = classifier.predict(mfccs)  ## Predicting new observation\n",
    "    vowel_results=[]\n",
    "    vowel_bools = attk.get_vowel_segments(clip_pathname)\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        if vowel_bools[i]==True:\n",
    "            vowel_results.append(results[i])\n",
    "\n",
    "    return np.mean(vowel_results) ## Vowels only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classify_clip(wav_pathname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "## Classifying a long audio file\n",
    "\n",
    "resolution_secs = 5.0\n",
    "\n",
    "os.chdir('/sharedfolder/')\n",
    "\n",
    "!wget -N https://github.com/hipstas/shaping-humanities-data/blob/master/audio/Fresh_Air_2017-07-31.mp3?raw=true -O Fresh_Air_2017-07-31.mp3\n",
    "\n",
    "import timeit\n",
    "tic=timeit.default_timer()\n",
    "\n",
    "media_path = \"/sharedfolder/Fresh_Air_2017-07-31.mp3\"\n",
    "\n",
    "snd = AudioFileClip.AudioFileClip(media_path)\n",
    "\n",
    "classifications = []\n",
    "\n",
    "for i in range(int(attk.duration(media_path)/resolution_secs)):\n",
    "    try:\n",
    "        snd.subclip(i * resolution_secs , (i * resolution_secs) + resolution_secs).write_audiofile('/tmp/temp_clip.wav')\n",
    "        classifications.append(classify_clip('/tmp/temp_clip.wav'))\n",
    "    except:\n",
    "        classifications.append(0.0)\n",
    "        print(\"Error: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Time elapsed: \"+str(timeit.default_timer() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Writing classification output to CSV\n",
    "\n",
    "classifier_threshold = 0.06\n",
    "\n",
    "classifier_output = []\n",
    "\n",
    "for classification in attk.smooth(np.array(classifications)):\n",
    "    if classification < classifier_threshold:\n",
    "        classifier_output.append(0)\n",
    "    if classification >= classifier_threshold:\n",
    "        classifier_output.append(1)\n",
    "\n",
    "csv_path = media_path[:-4]+'_mlpc2048_labels.csv'\n",
    "csv_path = media_path[:-4]+'_extratrees2048_labels.csv'\n",
    "\n",
    "with open(csv_path,'w') as fo:\n",
    "    for pair in attk.labels_to_ranges(classifier_output, label=1):\n",
    "        start = pair[0] * resolution_secs\n",
    "        duration = (pair[1] - pair[0]) * resolution_secs\n",
    "        fo.write(str(start) + ',' + str(start + duration) + ',Terry Gross\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
